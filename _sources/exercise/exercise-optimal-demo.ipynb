{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3068a6a-bb7e-412f-8aff-875afa0543cb",
   "metadata": {},
   "source": [
    "# Exercise 5 (demo) - Optimal interpolation\n",
    "\n",
    "\n",
    "**Aim:** To map temperature data based on individual Argo profile data into an x-y-t set of maps.\n",
    "\n",
    "**Data:** Data from early Argo profiles in the North Atlantic, provided in a `*.mat` file.\n",
    "\n",
    "**Directions:** Create an `*.ipynb` and figures of the mapped data.\n",
    "\n",
    "**Demo:** Since this is a more-involved calculation, most of the code has been provided for you below.  There are a few places where you need to make edits/tweaks and choices.  Some of these are indicated by the ellipses `...`.  Others simply say in the text `Try this`.\n",
    "\n",
    "**This exercise is based on one from a course by [Kathie Kelly](http://staff.washington.edu/kellyapl/Research.html) in 2008.**\n",
    "\n",
    "<hr>\n",
    "\n",
    "```{seealso}\n",
    "The method is introduced in: {cite:t}`Bretherton-etal-1976` A technique for objective analysis and design of oceanographic experiments applied to MODE-73.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d764246f-3015-41c6-a0e0-e9bc25c2c30f",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2019c0dd-b5e8-44cd-b840-7610db220bf5",
   "metadata": {},
   "source": [
    "### Create a notebook \n",
    "\n",
    "1. Create an `*.ipynb` containing the commands for this assignment, or copy this file.  \n",
    "    \n",
    "    ```{admonition} File naming convention\n",
    "    Name your python notebook something useful `ex<X>-<Lastname>-<slug>-seaocn.ipynb` where you replace `<X>` with the exercise number and `<slug>` with the short slug to name the topic, and `<Lastname>` with your last name.\n",
    "\n",
    "    Figures should be named something like `ex<X>fig<Y>-<Lastname>-<slug>-seaocn.png` where you replace `<X>` with the exercise number, `<Y>` with the figure number, and `<Lastname>` with your last name.\n",
    "    ```\n",
    "\n",
    "2. Import necessary packages.  \n",
    "\n",
    "\n",
    "\n",
    "    For example, `matplotlib` and `pandas` and `numpy` and `xarray`.  You may also need `scipy`.\n",
    "    ```\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import xarray as xr\n",
    "    from datetime import datetime\n",
    "    ```\n",
    "    If you are missing any of these packages, please refer to [Resources: Python](../resource/python).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a841f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import scipy.io as sio\n",
    "from matplotlib.patches import Rectangle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a44db0-ccfc-4bf5-87bf-0e09b9010344",
   "metadata": {},
   "source": [
    "### Load Matlab data\n",
    "\n",
    "3. Use the file `argo_med.mat` in the `data/` directory.  With the `scipy` package, you can load matlab-format data files into python.\n",
    "\n",
    "    ```{seealso}\n",
    "    Scipy io: [https://docs.scipy.org/doc/scipy/tutorial/io.html](https://docs.scipy.org/doc/scipy/tutorial/io.html)\n",
    "    ```\n",
    "\n",
    "4. Load the file.  Here, we're loading the matlab file into something called `mat_contents`.  Make a basic exploration. How big are the data?  What are the coordinates?\n",
    "\n",
    "    ```\n",
    "    mat_contents = sio.loadmat(fname)\n",
    "    print(mat_contents.keys())\n",
    "    print(mat_contents['time'])\n",
    "    ```\n",
    "    Since the functions you have available depend on the type of the variable, don't forget to use `type()` to find out what's in there.\n",
    "\n",
    "    For consistency with the code below, use the names `time`, `pres0`, `latd`, `lond`, `tprof`.\n",
    "\n",
    "\n",
    "    Note that the time is in Matlab `datenum` format, which is days since 0000-01-01.  Check out some tips on how to convert between Matlab time and python datenum.\n",
    "\n",
    "   ```{seealso}\n",
    "   - See [https://stackoverflow.com/questions/13965740/converting-matlabs-datenum-format-to-python](https://stackoverflow.com/questions/13965740/converting-matlabs-datenum-format-to-python) which has the example we can adapt as\n",
    "\n",
    "       datenums = time.flatten()\n",
    "       timestamps = pd.to_datetime(datenums-719529, unit='D')\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfe6ac-d842-44fa-94c7-bfec7ccf483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Change the path to match where you put the data\n",
    "file_path = '../data/'\n",
    "filename = 'argo_med.mat'\n",
    "fname = file_path + filename\n",
    "mat_contents = sio.loadmat(fname)\n",
    "\n",
    "# Pull out the individual data fields\n",
    "time = mat_contents['time']\n",
    "pres0 = mat_contents['pres0']\n",
    "latd = mat_contents['latd']\n",
    "lond = mat_contents['lond']\n",
    "tprof = mat_contents['tprof']\n",
    "\n",
    "# Create a variable `timestamps` which is in Python datetime format.\n",
    "# BUT, don't replace the `time` vector with this new one.  We will still use `time`\n",
    "datenums = time.flatten()\n",
    "timestamps = pd.to_datetime(datenums-719529, unit='D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa6179f-ba83-4a3e-a263-49aa0fee05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the time works.\n",
    "# The code below should produce the output `18-Jan-2003`\n",
    "tstr = timestamps.strftime(\"%d-%b-%Y\")\n",
    "tstr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c2ec76-648d-4513-8f3a-93029cd85a57",
   "metadata": {},
   "source": [
    "### Convert data to `xarray`\n",
    "\n",
    "We like `xarray` in this course, so we will re-bundle the data into an `xarray` object: a `DataSet`.  Below shows a way to do this which will throw an error.  Uncomment the lines and run it to see what the error looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8c524-4eb5-4388-a576-18b365d0539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_xr = xr.DataArray(data = tprof,\n",
    "#                       coords={'time': time, 'pres': pres0},\n",
    "#                       dims = [\"time\", \"pres\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3d5a5-17e7-4852-9f44-1dc1692916c7",
   "metadata": {},
   "source": [
    "### Error: `MissingDimensionsError`\n",
    "\n",
    "If you get an error like `MissingDimensionsError: cannot set variable 'pres' with 2-dimensional data without explicit dimension names. Pass a tuple of (dims, data) instead.`, this is a problem with the `shape` or size of your pressure or time dimensions.  Xarray wants these to be 1-dimensional.\n",
    "\n",
    "Try a `np.shape(time)` to see how big it is.\n",
    "\n",
    "If it is 1-dimensional, it will return something like\n",
    "```(1019,)```\n",
    "\n",
    "If it is returning ```(1, 1019)```, this is 2-dimensional, and xarray is having trouble with it.\n",
    "\n",
    "```{seealso}\n",
    "We dealt with problems of dimensionality in [Ex-tseries](exercise-tseries.ipynb).  The solution there was a little magical function called `squeeze()` available in `xarray`.  Our variable here is a `numpy` array, but there also exists the function `numpy.squeeze()`.  So, squeeze your variables to remove extraneous dimensions.\n",
    "```\n",
    "\n",
    "After squeezing everything, create several `xarray.DataArray` with the coordinates `time` and `pres`, where applicable.  For example:\n",
    "\n",
    "```\n",
    "data_xr2 = xr.DataArray(name = 'latd', data = latd,\n",
    "                        coords = {'time': time},\n",
    "                        dims = [\"time\"])\n",
    "```\n",
    "\n",
    "Once you have done this for each of `tprof`, `latd`, `lond`, and `timestamps`, you can merge them into a single `xarray.DataSet` using:\n",
    "\n",
    "    argo_med = xr.merge([data_xr,data_xr2,data_xr3,data_xr4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5bbfb-fd53-4860-b5cd-745aec333078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.  \n",
    "\n",
    "# Note that several of the below commands produce output, but youll only see the latest output print to the screen.\n",
    "# If you want to see the output of the others, encase it with `print(...)`\n",
    "type(tprof)\n",
    "np.shape(tprof)\n",
    "type(latd)\n",
    "np.shape(time)\n",
    "\n",
    "# Now squeeze\n",
    "time = time.squeeze()\n",
    "pres0 = ...\n",
    "latd = ...\n",
    "lond = ...\n",
    "np.shape(time)\n",
    "np.shape(pres0)\n",
    "\n",
    "# Now create a data array for each of `tprof`, `latd`, `lond`, and `timestamps`.  Use the coordinate `time` and dimensions `time` and `pres0`.\n",
    "data_xr = xr.DataArray(name = 'tprof', data = tprof,\n",
    "                       coords={'time': time, 'pres': pres0},\n",
    "                       dims = [\"time\", \"pres\"])\n",
    "data_xr2 = ...\n",
    "\n",
    "data_xr3 = ...\n",
    "\n",
    "data_xr4 = ...\n",
    "\n",
    "argo_med = xr.merge([data_xr,data_xr2,data_xr3,data_xr4])\n",
    "\n",
    "print(argo_med)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919bfa8f-fbe2-4d54-8d74-f34317eeeaff",
   "metadata": {},
   "source": [
    "### Plot data locations\n",
    "\n",
    "This is a quick-and-dirty plot, so `matplotlib` is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736636d-c6cf-4e6d-9167-8451b9d40e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(argo_med.lond, argo_med.latd,'*')\n",
    "plt.xlabel('Longitude [deg E]')\n",
    "plt.ylabel('Latitude [deg N]')\n",
    "plt.title('Argo profile locations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007faaf-35c7-4d50-a4cc-65b491017fbc",
   "metadata": {},
   "source": [
    "## Fig 1. Data locations\n",
    "\n",
    "```\n",
    "minlat = 40\n",
    "maxlat = 50\n",
    "minlon = 320\n",
    "maxlon = 340\n",
    "```\n",
    "\n",
    "- Draw a box on top of your figure to show where the selected region is.\n",
    "\n",
    "\n",
    "```\n",
    "rect = Rectangle((minlon, minlat), maxlon-minlon, maxlat-minlat, linewidth=1, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "```\n",
    "\n",
    "- Save the figure as Fig 1 for this exercise.  `ex5fig1-<LastName>-data_locations-seaocn.png`\n",
    "\n",
    "We'll work with data in an area with particularly high density of float profiles.\n",
    "\n",
    "Choose a pressure surface to work with. Note that your pressure vector is 6 elements long.  Pick one depth to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c953b55d-5daf-4e38-bb03-f8e2a2a27848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define region\n",
    "minlat = 40\n",
    "maxlat = 50\n",
    "minlon = 320\n",
    "maxlon = 340\n",
    "\n",
    "# Your code here\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(argo_med.lond, argo_med.latd,'*')\n",
    "plt.xlabel('Longitude [deg E]')\n",
    "plt.ylabel('Latitude [deg N]')\n",
    "plt.title('Argo profile locations')\n",
    "\n",
    "# Create a Rectangle patch & add to plot\n",
    "rect = Rectangle((minlon, minlat), maxlon-minlon, maxlat-minlat, linewidth=1, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# Save the figure to subdirectory figures/\n",
    "fig.savefig(\"figures/ex5fig1-Example-data_locations-seaocn.png\")\n",
    "\n",
    "# Figure out how big the datasets are\n",
    "temp = argo_med['tprof'][:,0]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84882335-f83c-4bdc-92d3-2bffd94e09c8",
   "metadata": {},
   "source": [
    "## Fig 2. Clip to a region and plot\n",
    "\n",
    "You've defined a maximum and minimum longitude and latitude above, and now you'd like to make your dataset smaller to work only within this region.\n",
    "\n",
    "`xarray` has a few options for finding/selecting/choosing subsets of a dataset.  These include `xr.sel()`, `xr.where()`, `xr.loc()`, etc.  They have some similarities and differences which we won't go into here.\n",
    "\n",
    "```{seealso}\n",
    "If you do a google search for `subset xarray by lat lon region`, one of the options that shows up (as of 17 April 2024) is:\n",
    "https://gis.stackexchange.com/questions/353698/how-to-clip-an-xarray-to-a-smaller-extent-given-the-lat-lon-coordinates\n",
    "\n",
    "There are a few solutions offered by the community.  Try them and see which works.\n",
    "```\n",
    "\n",
    "One suggested method:\n",
    "\n",
    "```\n",
    "mask_lon = (argo_med.lond >= minlon) & (argo_med.lond <= maxlon)\n",
    "```\n",
    "which creates a boolean vector which is True where *both* of these conditions are satisfied (the boolean operator `AND` or `&` requires both conditions to be satisfied.  The equivalent `OR` is `|`).  After creating this \"mask\", the `xarray.DataSet.where()` function can be used as:\n",
    "\n",
    "```\n",
    "argo_reg = argo_med.where(mask_lon & mask_lat, drop=True)\n",
    "```\n",
    "The option `drop=True` says to drop the elements of the dataset that do not satisfy the conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be1546-f770-4cdf-89f8-15dd74e6f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fails because our latitude and longitude vectors are 'variables' and time is the only coordinate\n",
    "mask_lon = (argo_med.lond >= minlon) & (argo_med.lond <= maxlon)\n",
    "mask_lat = ...\n",
    "\n",
    "argo_reg = ...\n",
    "\n",
    "# How large is the dataset now?\n",
    "print(argo_reg)\n",
    "\n",
    "# Your code here\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(argo_reg.lond,argo_reg.latd,'*')\n",
    "plt.xlabel('Longitude [deg E]')\n",
    "plt.ylabel('Latitude [deg N]')\n",
    "plt.title('Argo profile locations')\n",
    "ax = plt.gca()\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = Rectangle((minlon, minlat), maxlon-minlon, maxlat-minlat, linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "\n",
    "fig.savefig(\"figures/ex5fig2-Example-data_locations-seaocn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c44df-5d09-47e4-bc9d-4b1e0c78863d",
   "metadata": {},
   "source": [
    "### Remove mean & any obvious latitudinal dependence\n",
    "\n",
    "Here you should plot against latitude and fit a polynomial (1 degree, if that suffices to remove most of the signal).  Note that a 1-degree polynomial will also remove the mean, so you don't need to separately remove the mean of these values.  To check the mean of the dataset, use the `plt.hist()` command from `matplotlib`.\n",
    "```\n",
    "plt.hist(argo_reg.tprof.values.flatten())\n",
    "```\n",
    "\n",
    "Then check for a latitudinal dependence.  For example:\n",
    "```\n",
    "plt.plot(argo_reg.latd,argo_reg.tprof[:,3],'*')\n",
    "```\n",
    "\n",
    "This figure does not need to be saved - it is a quick and dirty plot to take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746246b-410f-4c12-b2da-f06018643ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].hist(argo_reg.tprof.values.flatten())\n",
    "ax[1].plot(argo_reg.latd,argo_reg.tprof[:,3],'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97131298-7de0-4c91-8fa6-8e14195eb926",
   "metadata": {},
   "source": [
    "## Fig 3. Compute latitudinal temperature dependence\n",
    "\n",
    "Recall from [exercise 2a tseries](exercise-tseries.ipynb) how to fit a 1-degree polynomial.\n",
    "\n",
    "1. Extract data from one pressure level.  Note that the second dimension or axis of the `tprof` dataArray is not unit-length.  Pick one depth surface to use for the later exercises.  Call this vector `temp1` for consistency with later code examples.\n",
    "\n",
    "2. Remove the NaN-values (if any).  For example, find the indices where there are NaN-values using `np.isnan`, then subselect only the good stuff from `temp1` using it in `temp1[~np.isnan(temp1)]`.  Remove these from latitude as well.\n",
    "\n",
    "    ```\n",
    "    ikeep = ~np.isnan(temp1)\n",
    "    temp1 = temp_anom[ikeep]\n",
    "    ```\n",
    "\n",
    "3. Fit a 1-d polynomial to `temp1` as a function of `lat1` (latitude, with the same NaN-values removed).\n",
    "\n",
    "    ```\n",
    "    pf = np.polyfit(lat1, temp1, 1)\n",
    "    xp = np.linspace(minlat,maxlat,20)\n",
    "    p1 = np.poly1d(np.polyfit(lat1,temp1,1))\n",
    "    ```\n",
    "\n",
    "5. Remove the polynomial fit from the original data to create anomalies\n",
    "\n",
    "    ```\n",
    "    lat_model = p1(lat1)\n",
    "    temp_anom[ikeep] = temp1 - lat_model\n",
    "    ```\n",
    "\n",
    "6. Create a new `xarray.DataArray` containing `temp_anom` then merge it using `xr.merge` back into `argo_reg`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c2d01a-c415-44a4-86bb-cb7bbeabdcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a line\n",
    "lat1 = argo_reg.latd.values\n",
    "# This is picking a specific depth.  What depth is it at?\n",
    "temp1 = argo_reg.tprof[:,3].values\n",
    "\n",
    "# Our new data vector will be called `temp_anom`\n",
    "temp_anom = temp1\n",
    "ikeep = ~np.isnan(temp1)\n",
    "\n",
    "lat1 = lat1[ikeep]\n",
    "temp1 = temp1[ikeep]\n",
    "\n",
    "# Fit a polynomial\n",
    "pf = np.polyfit(lat1, temp1, 1)\n",
    "xp = np.linspace(40,50,20)\n",
    "p1 = np.poly1d(np.polyfit(lat1, temp1, 1))\n",
    "\n",
    "# Calculate anomalies relative to the latitudinal gradient\n",
    "lat_model = p1(lat1)\n",
    "temp_anom[ikeep] = temp1-lat_model\n",
    "\n",
    "# Merge these back into the original\n",
    "# See what else needs to be used by checking the cell above\n",
    "data_xr4 = xr.DataArray(name = 'temp_anom', ...)\n",
    "\n",
    "# Merge into the same dataset\n",
    "argo_reg = xr.merge([argo_reg,...])\n",
    "\n",
    "print(argo_reg) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb541c-c3db-41fe-b94b-1bfab65e029c",
   "metadata": {},
   "source": [
    "### Now plot the process of fitting a line\n",
    "\n",
    "And save as your 3rd figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7e3d5-3bfe-4fa5-bbc2-37e58bd42634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "fig, ax = plt.subplots(2,1)\n",
    "\n",
    "# Determine latitude dependence\n",
    "ax[0].plot(argo_reg.latd,argo_reg.tprof[:,3],'o')\n",
    "ax[0].plot(lat1,temp1,'r+')\n",
    "ax[0].plot(xp,p1(xp))\n",
    "\n",
    "ax[1].plot(lat1, temp_anom[ikeep],'*')\n",
    "\n",
    "# Annotate figure\n",
    "ax[1].set_xlabel('Latitude [deg N]')\n",
    "ax[1].set_ylabel('Temperature anomaly [deg C]')\n",
    "ax[0].set_ylabel('Temperature [deg C]')\n",
    "\n",
    "\n",
    "fig.savefig(\"figures/ex5fig3-Example-data_locations-seaocn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6edce-6ce0-4077-abbd-88f7ae2a4d79",
   "metadata": {},
   "source": [
    "### (Optional) Remove outliers larger than 3 standard deviations\n",
    "\n",
    "1. Compute the standard deviation of your data vector\n",
    "\n",
    "   ```\n",
    "   std1 = np.nanstd(temp_anom)\n",
    "   ```\n",
    "\n",
    "2. Create a mask for where these data are outliers (by a 3x std definition)\n",
    "\n",
    "    ```\n",
    "    mask_outlier = (argo_reg.temp_anom > -3*std1) | (argo_reg.temp_anom > 3*std1) \n",
    "    ```\n",
    "\n",
    "3. Subselect the `argo_reg` for the non-outliers and save it into `argo_reg_good`.\n",
    "\n",
    "    ```\n",
    "    argo_reg_good = argo_reg.where(mask_outlier, drop=True)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44fa06f-276c-4a2b-92cc-dfaca18b342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you don't do the optional, you still need the newly named `argo_reg_good` for later code.\n",
    "argo_reg_good = argo_reg\n",
    "\n",
    "# Remove the outliers (optional)\n",
    "# Uncomment and complete the lines\n",
    "#std1 = ...\n",
    "#mask_outlier = ...\n",
    "#argo_reg_good = ...\n",
    "\n",
    "\n",
    "# Print something to the screen to tell how many datapoints were removed (if any)\n",
    "mp = argo_reg_good.temp_anom.shape[0]\n",
    "mq = argo_reg.temp_anom.shape[0]\n",
    "\n",
    "print('Removed ' + str(mq-mp) + ' values out of a total of ' + str(mq) + ' (' + str(np.around(100*(mq-mp)/mq,2)) + '% of data points removed)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad64cc80-cb77-4661-a5f7-36e71d83910c",
   "metadata": {},
   "source": [
    "## Computing the covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f6050-c07a-4d60-88df-0f7f8d67bf44",
   "metadata": {},
   "source": [
    "### Calculate pairwise differences\n",
    "\n",
    "For every Argo measurement, calculate the difference to every other Argo measurement, and the difference in location (dx) and time (dt)\n",
    "\n",
    "Read through the code below to understand what is going on.\n",
    "\n",
    "- How are new vectors being initialised?  What is their initial size?  How does there size get increased in the loop?\n",
    "\n",
    "- What are the indices of the for-loops?  What values will ``jj`` (the outer loop index) take?  What about ``ii``?   You can check this by adding some lines of code in relevant places within the loop:\n",
    "\n",
    "    ```\n",
    "    print(jj)\n",
    "    ```\n",
    "    Make a *new cell* and try the very simple loop:\n",
    "    ```\n",
    "    for jj in range(0,5):\n",
    "        print(jj)\n",
    "    ```\n",
    "    What is the largest value of `jj`?  How does this compare to the inputs you put in `range()`?\n",
    "  \n",
    "\n",
    "- What is the `theta` variable?  What are we using it for?\n",
    "\n",
    "- What is the factor `np.cos(theta...` doing?  Why is there an `np.pi/180` inside?\n",
    "\n",
    "- How big is ``index`` after executing the loop?  Note that this variable is only counting for you - it's not doing anything else in this loop (but we will use it later).  How did we increment it?  What would happen if you changed the line to\n",
    "\n",
    "    ```\n",
    "    index += 2\n",
    "    ```\n",
    "\n",
    "- Did you notice the indentation used on the loops?  These are nested loops, so the activity inside the 2nd loop is indented twice.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed277e74-4a5d-4e16-b9c9-18a3c6c667cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell for your simple loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f04c03-4032-41db-a5db-33a441057159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the vectors will be ndat^2/2, so this gets pretty big\n",
    "ndat = argo_reg_good.temp_anom.shape[0]\n",
    "\n",
    "# Initialise some empty vectors\n",
    "dist = np.empty((0,0))\n",
    "delt = np.empty((0,0))\n",
    "temp_diff = np.empty((0,0))\n",
    "myind = np.empty((0,0))\n",
    "index = 0\n",
    "\n",
    "for jj in range(0,ndat):\n",
    "    for kk in range(jj+1,ndat): \n",
    "        dt = np.abs(argo_reg_good.time[jj] - argo_reg_good.time[kk])\n",
    "        dlat = argo_reg_good.latd[jj] - argo_reg_good.latd[kk]\n",
    "        dlon = argo_reg_good.lond[jj] - argo_reg_good.lond[kk]\n",
    "\n",
    "        theta = np.mean([argo_reg_good.latd[jj], argo_reg_good.latd[kk]])\n",
    "\n",
    "        # Distance\n",
    "        dist1 = np.sqrt(dlat**2 + (dlon * np.cos(theta*np.pi/180))**2)*111\n",
    "        dist = np.append(dist, dist1)\n",
    "\n",
    "        # Time difference\n",
    "        delt = np.append(delt,dt)\n",
    "        \n",
    "        # Temperature difference\n",
    "        dif_temp1 = argo_reg_good.temp_anom[jj] - argo_reg_good.temp_anom[kk]\n",
    "        temp_diff = np.append(temp_diff, dif_temp1)\n",
    "\n",
    "        # \n",
    "        myind = np.append(myind,index)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd49fb6-ad0a-4f3b-bdd3-40ac5b423a52",
   "metadata": {},
   "source": [
    "### Create a new `xarray` to hold the results\n",
    "\n",
    "The data fields should include `time_diff`, `distance`, `temp_diff`, against the index `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a773f9f-ef78-4a84-94b8-201b95642651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an xarray\n",
    "data_xr = xr.DataArray(name = 'time_diff', data = delt,\n",
    "                       coords={'index': myind},\n",
    "                       dims = [\"index\"])\n",
    "data_xr2 = xr.DataArray(name = 'distance', data = dist,\n",
    "                        coords= {'index': myind},\n",
    "                        dims = [\"index\"])\n",
    "data_xr3 = xr.DataArray(name = 'temp_diff', data = temp_diff,\n",
    "                        coords = {'index': myind},\n",
    "                        dims = [\"index\"])\n",
    "\n",
    "data_covar = xr.merge([data_xr, data_xr2, data_xr3])\n",
    "print(data_covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eadb31b-d3b9-49bb-ac34-b910bb183239",
   "metadata": {},
   "source": [
    "## Calculating the structure function\n",
    "\n",
    "$$\n",
    "<(T_1-T_2)^2> = <T_1^2> + <T_2^2> - 2<T_1T_2> = 2<T^2> + 2<err^2> - 2<T_1T_2>\n",
    "$$\n",
    "\n",
    "where $<\\cdot>$ denotes an average or mean.  The structure function $S$ is the variance + squared error,\n",
    "\n",
    "$$\n",
    "S = \\frac{1}{2}<(T_1-T_2)^2>\n",
    "$$\n",
    "\n",
    "In python, this becomes\n",
    "```\n",
    "delt_squared = data_covar.temp_diff**2/2\n",
    "```\n",
    "which is one half the squared temperature difference, but without the averaging in $S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05411ad9-2932-44d3-b2cf-6f58c08b73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "delt_squared = data_covar.temp_diff**2/2\n",
    "\n",
    "data_xr4 = xr.DataArray(name = 'deltsqr', data = delt_squared,\n",
    "                        coords = {'index': myind},\n",
    "                        dims = [\"index\"])\n",
    "data_covar = xr.merge([data_covar, data_xr4])\n",
    "\n",
    "nprs = delt.shape[0]\n",
    "\n",
    "# We will bin the differences as a function of space and time\n",
    "# Choose the width of each bin\n",
    "ds_bin = 25 # Spatial  - Use something like 10 - 100 kilometers (you can try a few)\n",
    "\n",
    "# Choose a maximum distance to use\n",
    "maxd = 900 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e25439-a16b-4ce0-8f3d-4c1461e8b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset is large, initially only work where the difference in time is small\n",
    "itime_small = delt<dt_bin\n",
    "len(itime_small)\n",
    "type(itime_small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fbd846-33c7-4f06-ad32-c5f4b763f8d1",
   "metadata": {},
   "source": [
    "### Bin the structure function as a function of distance\n",
    "\n",
    "Create the variable $S(x)$ which is a binned estimate of the squared temperature differences, as a function of distance $x$.  We will call this `S_dx` in python, against the distance vector `ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d0ce2-5fca-4f80-90b3-fa474268116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask for nearby in time and in space\n",
    "# There is no point in running this over the entire dataset since far away in space/time are unlikely to influence values at a point\n",
    "mask_small_dt = (data_covar.time_diff < dt_bin)\n",
    "mask_small_ds = (data_covar.distance < maxd)\n",
    "data_covar_near = data_covar.where(mask_small_dt & mask_small_ds, drop=True)\n",
    "\n",
    "print(data_covar_near.distance.values.max())\n",
    "\n",
    "# Subset the distances and the other values for small time differences\n",
    "\n",
    "# Length of vector for distances\n",
    "ds = np.arange(ds_bin/2, maxd, ds_bin)\n",
    "# Initialise the vector\n",
    "S_dx = np.empty((0,0))\n",
    "\n",
    "# Loop through the distance vector `ds`\n",
    "for jj in range(len(ds)):\n",
    "    # Create the lower and upper distance limits within which to compute the average\n",
    "    dsmall = ds[jj]-ds_bin/2\n",
    "    dlarge = ds[jj]+ds_bin/2\n",
    "\n",
    "    # Select for data within these limits\n",
    "    dt1 = data_covar_near.where((data_covar_near.distance >= dsmall) & (data_covar_near.distance < dlarge), drop=True)\n",
    "\n",
    "    # Calculate the average\n",
    "    smean1 = np.mean(dt1.deltsqr)\n",
    "\n",
    "    # Append the value to the S_dx vector\n",
    "    S_dx = np.append(S_dx,smean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404041e-fd1c-4d56-952f-782c2d71a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the binned structure function\n",
    "\n",
    "# Variance /error increases with distance\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(ds,S_dx)\n",
    "ax.set_xlabel('Distance [km]')\n",
    "ax.set_ylabel('$T_{rms}^2$ [deg C^2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ffe523-85f2-4386-b586-f60359943910",
   "metadata": {},
   "source": [
    "### Estimate noise and variance\n",
    "\n",
    "Where noise (errsq), signal variance (var_signal), total variance (var_total) are in the equation for the structure function as\n",
    "\n",
    "$$\n",
    "2S = <T1^2> + <T2^2> -2<T1*T2> = 2<T^2> + 2<err^2> -2<T1*T2>\n",
    "$$\n",
    "\n",
    "for delt=0, this is the squared error.\n",
    "\n",
    "for delt large, this is the total variance (signal variance plus the squared error).  \n",
    "\n",
    "You can repeat the above calculations changing the mask_small_dt values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ede731-328a-436f-af0e-b74db18bea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with some values here that will affect your fit in the later code.\n",
    "errsqr = 1\n",
    "var_total = 1.2\n",
    "var_signal = var_total - errsqr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9cb35a-903a-458d-aa05-3d6dc22d645d",
   "metadata": {},
   "source": [
    "### Fit a covariance function\n",
    "\n",
    "The covariance function must be estimated either from the data to be mapped or from independent data. The function should be as simple as possible while still describing the essential variations in the data. Generally, an analytic function is used to allow for simple computation for a variety of spatial (and temporal) separations.  The covariance function should decrease with increasing distance (or time), which minimizes the possibility that (5) will produce unrealistic (e.g., negative) squared error estimates, another reason for removing periodic signals. Included in the covariance function is an estimate of the variance of the field, which may vary somewhat spatially. Finally, an estimate is needed of the random error in the measurements, in addition to the error associated with not having a measurement at the exact location (or time) of the grid points for the map.\n",
    "\n",
    "Here we will use:\n",
    "\n",
    "$$\n",
    "cov=var_{signal}*\\exp(-ds^2/L_s^2-dt/L_t)\n",
    "$$\n",
    "\n",
    "In your calculations below, guess initial decorrelation length scale $L_s=300$ km, and decorrelation time scale of 10 days, $L_t=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5131e-8279-4a4c-b96a-a533bb8120ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some initial guesses for the decorrelation scales\n",
    "# These are the distances in kilometers and days over which you expect data to be similar (or to have an influence on nearby datapoints in the map)\n",
    "Ls = ...\n",
    "Lt = ...\n",
    "\n",
    "\n",
    "# Define a simple covariance function as a function of distance\n",
    "def my_covar_x(var_signal,ds,Ls):\n",
    "    covx = var_signal*np.exp(-(ds/Ls)**2)\n",
    "    \n",
    "    return covx\n",
    "\n",
    "# Adjusting the value from the cell above to better fit the curve.\n",
    "var_sig2 = .9\n",
    "\n",
    "# Covariance\n",
    "cov_ds = var_total - S_dx\n",
    "\n",
    "# Calculate the decorrelation curve\n",
    "covx = my_covar_x(var_sig2, ds, Ls)\n",
    "\n",
    "# Plot it against the structure function\n",
    "fig,ax = plt.subplots()\n",
    "plt.plot(ds,cov_ds)\n",
    "plt.plot(ds,covx,'*')\n",
    "ax.set_xlabel('Distance [km]')\n",
    "ax.set_ylabel('Temperature covariance [(deg C)$^2$]')\n",
    "ax.set_title('For Ls = ' + str(Ls) + 'km')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a404d-a656-4d0e-8656-3a02b9f75488",
   "metadata": {},
   "source": [
    "## Bin the structure function as a function of distance\n",
    "\n",
    "Repeat for distance, experimenting with values for `dt_bin` and `maxt`.  Note that there is no point in a very large `maxt` value since the dataset include data all collected within about 6 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf871bd9-9153-4d74-b704-0f21d9d62c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask for nearby in time and in space\n",
    "dt_bin = ... # Temporal - Use something like 10 - 60 days (you can try a few)\n",
    "maxt = 150\n",
    "\n",
    "mask_small_dt = (data_covar.time_diff < maxt)\n",
    "mask_small_ds = (data_covar.distance < ds_bin*2)\n",
    "data_covar_near = data_covar.where(mask_small_dt & mask_small_ds, drop=True)\n",
    "\n",
    "print(data_covar_near.distance.values.max())\n",
    "\n",
    "# Subset the distances and the other values for small time differences\n",
    "# Length of vector for distances\n",
    "dt = np.arange(dt_bin/2, maxt, dt_bin)\n",
    "S_dt = np.empty((0,0))\n",
    "for jj in range(len(dt)):\n",
    "    dsmall = dt[jj]-dt_bin/2\n",
    "    dlarge = dt[jj]+dt_bin/2\n",
    "    dt1 = data_covar_near.where((data_covar_near.time_diff >= dsmall) & (data_covar_near.time_diff < dlarge), drop=True)\n",
    "    smean1 = np.nanmean(dt1.strfcn)\n",
    "    S_dt = np.append(S_dt,smean1)\n",
    "\n",
    "# Plot the binned structure function\n",
    "# Variance /error increases with distance\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(dt,S_dt)\n",
    "ax.set_xlabel('Time differential [days]')\n",
    "ax.set_ylabel('$T_{rms}^2$ [deg C^2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6701e-7c39-4517-b207-dfdc670c059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit a covariance function\n",
    "Ls = ...\n",
    "Lt = ...\n",
    "cov_dt = var_total - S_dt\n",
    "\n",
    "# Simple covariance in time\n",
    "def my_covar_t(var_signal,ds,Ls):\n",
    "    covt = var_signal*np.exp(-dt/Lt)\n",
    "    \n",
    "    return covt\n",
    "\n",
    "covt = my_covar_t(var_sig2, dt, Lt)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "plt.plot(dt,cov_dt)\n",
    "plt.plot(dt,covt,'*')\n",
    "ax.set_xlabel('Time differential [days]')\n",
    "ax.set_ylabel('Temperature covariance [(deg C)$^2$]')\n",
    "ax.set_title('For Lt = ' + str(Lt) + 'days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eba928-99df-4d1a-aed5-70525281b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined time and space decorrelation\n",
    "def my_covar_xt(var_signal,ds,Ls,dt,Lt):\n",
    "    covxt = var_signal*np.exp(-(ds/Ls)**2-dt/Lt)\n",
    "\n",
    "    return covxt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08000a67-4db8-472e-91fe-872ab60f2487",
   "metadata": {},
   "source": [
    "# Optimal interpolation\n",
    "\n",
    "Optimal interpolation (or “objective mapping,” as it is frequently called in oceanography, or “kriging” in geophysics) is a procedure for obtaining a map from observations that has the minimum squared error, based on the statistics of the field to be mapped {cite:t}`Bretherton-etal-1976`. The unique aspect of this procedure is the error map that accompanies the map of the desired variable.\n",
    "\n",
    "Why compute an objective map?\n",
    "- to get a gridded field from irregularly spaced data (for data assimilation or for forcing a model)\n",
    "- to get a field with which to derive other estimates (e.g., geostrophic velocity from fields of density)\n",
    "- to get an estimate of the errors associated with a set of measurements at various locations and times \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad71874-9d9a-4d2b-a9a5-76aa154e50af",
   "metadata": {},
   "source": [
    "## Deriving the objective estimate\n",
    "\n",
    "Given a set of measurements at arbitrary locations (and times), $D(x,y,t)$, we want to create a series of maps at regular locations (and times).  The measurement locations may be fixed, as in measurements from moored instruments, random as from Argo floats, or repeating but not on a uniform grid, as for satellite observations. The problem is to find the best linear combination of observed values for an estimate at a fixed location and time\n",
    "\n",
    "$$\n",
    "\\hat{d}(x_0,y_0,t_0) = D\\alpha\\qquad\\qquad (1)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a vector of coefficients that are applied to the matrix of observations $D$.  The columns of $D$ are time series at the various location $(x,y)$. The best estimate is one that minimizes the square of the error\n",
    "\n",
    "$$\n",
    "\\epsilon = \\hat{d}(x_0,y_0,t_0)-d_T(x_0,y_0,t_0)\\qquad\\qquad (2)\n",
    "$$\n",
    "\n",
    "where $d_T$ is the actual value of the variable at that location and time.\n",
    "\n",
    "To insure the smallest squared error, we require that the error be orthogonal to the estimate, that is,\n",
    " \n",
    "\n",
    "This statement requires some interpretation, because it assumes that we have a time series (or an ensemble) of estimates of the variable and therefore an ensemble of errors, with which to determine $\\alpha$. For simplicity, take a hypothetical case in which data comes from fixed locations (as from a mooring) with no gaps (imagine!). Then at any given location the coefficients $\\alpha$ will not change in time and we have the times series for (2). Combining (1) and (2) gives\n",
    "\n",
    "$$\n",
    "(D\\alpha)^T(D\\alpha-d_T)=0\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\alpha^T D^T D\\alpha = \\alpha^T D^T d_T\n",
    "$$\n",
    "\n",
    "where $C$ is the data-data covariance matrix, then\n",
    "\n",
    "$$\n",
    "\\alpha^T C\\alpha = \\alpha^T z \\qquad\\qquad (3)\n",
    "$$\n",
    "\n",
    "and $N$ is the length of the time series in $D$, so that\n",
    "\n",
    "$$\n",
    "\\frac{D^TD}{N} = C\n",
    "$$\n",
    "\n",
    "that is, the covariance between all the observations (at each of the moorings) with each other and $z$ is a vector of covariances between the observations and the actual value $d_T$, as\n",
    "\n",
    "$$\n",
    "z = \\frac{D^Td_T}{N}\n",
    "$$\n",
    "\n",
    "We can solve for the coefficients $\\alpha$ explicitly as\n",
    "\n",
    "$$\n",
    "\\alpha = C^{-1}z \\qquad\\qquad (4)\n",
    "$$\n",
    "\n",
    "For the actual calculation in python, we use \n",
    "```                \n",
    "alpha = np.linalg.solve(C,zz)\n",
    "```\n",
    "\n",
    "It seems reasonable to estimate $C$ from the observations (at least for the mooring example), but how do we get $z$ when we do not know the actual value of the variable? In practice, we approximate the covariances using a simple (analytic) function.   For a spatial map, the covariance function may simply be a function of the distance between the data locations, or it might vary in the $x$ and $y$-directions, if their statistics differ significantly. In addition to the spatial separation, it might include the temporal separation. So, the covariance matrix $C$ between data at different locations is just a function of their separation. Similarly, the covariance matrix $z$ is just a function of the distance between the observations (here, the moorings) and the grid point in the map.\n",
    "\n",
    "To understand the form of the coefficients (4), note that the “numerator” $z$ depends on how close the data are to the grid point $(x_0,y_0)$, that is, the data that are nearest to the grid point are weighted most heavily in the estimate (1). What about the denominator $C$?  The data covariance matrix describes how well the data are correlated with each other, that is, the extent to which observations that are close to each other are redundant or, conversely, that widely separated observations are independent.  The denominator can be seen as a correction to the weighting by distance from the grid point (the numerator) to account for data redundancy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5415a0-be08-44a5-947c-586fcb55e272",
   "metadata": {},
   "source": [
    "\n",
    "## The error estimate\n",
    "\n",
    "What is the (squared) error associated with this estimate?\n",
    "\n",
    "$$\n",
    "\\epsilon^T\\epsilon = \\left(\\hat{d}-d_T\\right)^T\\left(\\hat{d}-d_T\\right)\n",
    "= \\hat{d}^T\\hat{d} - 2\\hat{d}^Td_T + d_T^Td_T\n",
    "$$\n",
    "\n",
    "Using (1) and (3) we can combine the first and second terms (on the right) to get\n",
    "\n",
    "$$\n",
    "\\epsilon^T\\epsilon = d_T^T d_T-  \\hat{d}^Td_T = d_T^Td_T - \\alpha^TD^Td_T\n",
    "$$\n",
    "\n",
    "where the first term is the *data variance* (times $N$) and the second term can be simplified using (4) to get \n",
    "\n",
    "$$\n",
    "\\left<\\epsilon^2\\right>=\\sigma_d^2-\\alpha^Tz \\qquad\\qquad (5)\n",
    "$$\n",
    "\n",
    "where the first term on the RHS is the data variance. For observations that are poorly correlated with the location of a grid point in the map, the squared error can approach the variance. In this case, the estimate is not useful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41a69b-6292-4399-b73f-d20b90e0ed89",
   "metadata": {},
   "source": [
    "## Removing the mean and periodic signals\n",
    "\n",
    "The objective map is performed on random variables whose statistics can be described by a covariance function. Generally, if the data is a function of time $d(x,t)$, the temporal mean is removed, because of its implicit association with the ensemble average. By removing the mean and mapping the anomaly, the mapping program will not have to try to recover the mean value using sparse data. If the expected squared error is large, the anomaly can then be set to zero. A similar argument can be made for a periodic signal, if it is reasonably well-defined either by climatology or a long data record. The mapped anomaly can be added to the mean (and periodic signal) to recover the total value of the variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ca03a-0042-4413-a0d1-9ebb9963b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_map_2D(vart, var, smax, Ls, tmax, Lt, data, longrid, latgrid, tmap):\n",
    "    # 2D objective map using data(lon,lat,time)\n",
    "    #\n",
    "    # Note: temporal mean of data should be removed first\n",
    "    # compute 2D Gauss-Markov (objective) estimate \n",
    "    #\n",
    "    # data - xarray of location, time, and observed value\n",
    "    #\t variables lond, latd, time and temp_anom\n",
    "    # longrid, latgrid - output locations for maps\n",
    "    # tmap - time of output map - in Matlab time, float since 0000-01-01 with\n",
    "    # increment of days, i.e. 0000-01-02 is 1.\n",
    "    #\n",
    "    # data variance (vart) is given by\n",
    "    #  vart = var + noise^2\n",
    "    #\n",
    "    # error is in units of signal (NOT fractional)\n",
    "    #\n",
    "    # For each output grid point:\n",
    "    #\t1) find data within within smax and tmax of grid pt & tmap \n",
    "    #\t2) compute data-data covariance matrix \n",
    "    #\t3) compute data-xgrid covariance vector zz\t\t\t\t\n",
    "    #\t4) compute the coefficients \n",
    "    #\t\talpha = inv(C) * zz      (alpha=C\\zz)\n",
    "    #\t5) compute the estimate for each grid point\n",
    "    #\t\tdhat=dij*alpha\n",
    "    #\t6) compute the normalized error estimate\n",
    "    #\t\terr^2 = var - zz'*alpha \n",
    "    #\t7) check for instabilities in error calculation & replace with closest \n",
    "    #      single datum, if necessary\n",
    "    #\n",
    "    [ny, nx] = longrid.shape\n",
    "    # Unpack the input data\n",
    "    lon_data = data.lond\n",
    "    lat_data = data.latd\n",
    "    time_data = data.time # Note these are in Matlab datenum format (days since 0000-01-01)\n",
    "    val_data = data.temp_anom\n",
    "\n",
    "    err0 = np.sqrt(vart);\n",
    "\n",
    "    # Initialise the mapped data & error\n",
    "    mapdata = np.empty((ny, nx))\n",
    "    err_rms = err0*np.ones(mapdata.shape)\n",
    "\n",
    "    # Conversions for lat to kilometers\n",
    "    yscl = 111\n",
    "    cs = np.cos(np.mean(lat_data)*np.pi/180)\n",
    "\n",
    "    # Cycle through positions in the grid\n",
    "    for jj in range(0,ny):\n",
    "        \n",
    "        for ii in range(0,nx):\n",
    "            # Find distance (km) from data to the map grid point at\n",
    "            # latgrid[jj,ii] and longrid[jj,ii]\n",
    "            # Note: data_lon and data_lat are vectors of the same length\n",
    "            dr = yscl * np.sqrt(cs * (lon_data - longrid[jj,ii])**2 + (lat_data - latgrid[jj,ii])**2)\n",
    "            # Find the time difference between the time for the map (tmap) and of the data (time_data)\n",
    "            dt = abs(time_data - tmap)\n",
    "    \n",
    "            # Boolean mask for distances < smax and time diff < tmax\n",
    "            mask_near = (dr < smax) & (dt < tmax)\n",
    "\n",
    "            # How many data points are near enough to be used?\n",
    "            npt = mask_near.sum().values\n",
    "\n",
    "            # If there are any original datapoints nearby in time/space, then continue\n",
    "            if npt>0:\n",
    "                # Subselect the input data for only those points near in time/space\n",
    "                lat_near_ij = lat_data.where(mask_near, drop=True).values\n",
    "                lon_near_ij = lon_data.where(mask_near, drop=True).values\n",
    "                time_near_ij = time_data.where(mask_near, drop=True).values\n",
    "                val_near_ij = val_data.where(mask_near, drop=True).values\n",
    "                dr_near_ij = dr.where(mask_near, drop=True).values\n",
    "                dt_near_ij = dt.where(mask_near, drop=True).values\n",
    "\n",
    "                # Compute data-grid covariance vector\n",
    "                zz = my_covar_xt(var,dr_near_ij,Ls,dt_near_ij,Lt)\n",
    "                \n",
    "                # Compute data-data covariance matrix\n",
    "                # Initialise an empty matrix, sized npt x npt square matrix\n",
    "                C = np.empty((npt,npt))\n",
    "\n",
    "                for iii in range(0,npt):\n",
    "                    lat_data1 = lat_near_ij[iii]\n",
    "                    lon_data1 = lon_near_ij[iii]\n",
    "                    time_data1 = time_near_ij[iii]\n",
    "                    \n",
    "                    for jjj in range(0,npt):\n",
    "                        lat_data2 = lat_near_ij[jjj]\n",
    "                        lon_data2 = lon_near_ij[jjj]\n",
    "                        time_data2 = time_near_ij[jjj]\n",
    "\n",
    "                        # Compute distance between pairs of data points\n",
    "                        dr_data = yscl*np.sqrt(cs * (lon_data2 - lon_data1)**2+(lat_data2 - lat_data1)**2)\n",
    "\n",
    "                        # Compute delta time between the data points\n",
    "                        dt_data = abs(time_data2 - time_data1)\n",
    "\n",
    "                        # Covariance matrix\n",
    "                        C[iii,jjj] = my_covar_xt(var,dr_data,Ls,dt_data,Lt)\n",
    "    \n",
    "                # Compute map value and error estimate\n",
    "                alpha = np.linalg.solve(C,zz)\n",
    "    \n",
    "                # Map the data values into the map locations\n",
    "                dhat = val_near_ij.conj().transpose() @ alpha\n",
    "                # Estimate the error\n",
    "                ersqs = vart - zz.conj().transpose() @ alpha\n",
    "    \n",
    "                # Value & error at nearest point (max covariance)\n",
    "                zz0 = zz.max()\n",
    "                kk = zz.argmax()\n",
    "                err1 = vart - zz0**2/var\n",
    "    \n",
    "                # Check for instability:\n",
    "                # 1) estimated error > error for single nearest point, or\n",
    "                # 2) squared error < 0\n",
    "                # For one datum: alpha = zz0/var\n",
    "                if (ersqs > err1) | (ersqs < 0):\n",
    "                    dij = dij(kk) # at max zz\n",
    "                    ersqs = err1\n",
    "                    alpha = zz0/var\n",
    "                    dhat = val_near_ij.conj().transpose() @ alpha\n",
    "\n",
    "                # Map values\n",
    "                mapdata[jj,ii] = dhat\n",
    "                err_rms[jj,ii] = np.sqrt(ersqs)\n",
    "            else:\n",
    "                mapdata[jj,ii] = 0\n",
    "                err_rms[jj,ii] = 0\n",
    "\n",
    "    return mapdata, err_rms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9bebe-da6e-4364-8f45-cd4977501ecf",
   "metadata": {},
   "source": [
    "## Run the optimal interpolation\n",
    "\n",
    "Loop through time (months).  \n",
    "\n",
    "Add comments to the code below to describe what each section or line is doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72a837-9f04-4614-a1b1-41965644a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run obj map routine \n",
    "# map 1 month at a time\n",
    "# Define the maximum space/time intervals over which to grid\n",
    "smax = .7*Ls\n",
    "tmax = .7*Lt\n",
    "\n",
    "# Define a map grid\n",
    "latg = np.arange(minlat, maxlat, 1)\n",
    "long = np.arange(minlon, maxlon, 1)\n",
    "\n",
    "[xgrid, ygrid] = np.meshgrid(long,latg)\n",
    "\n",
    "d1 = my_covar_xt(var_signal,smax,Ls,0,Lt)     # max space lag, no time lag\n",
    "d2 = my_covar_xt(var_signal,0,Ls,tmax,Lt)     # max time lag, no space lag\n",
    "\n",
    "data = argo_reg_good\n",
    "\n",
    "epoch0 = mdates.date2num(np.datetime64('0000-01-01'))\n",
    "epoch1 = mdates.date2num(np.datetime64('1970-01-01')+1)\n",
    "\n",
    "vart = var_total\n",
    "var = var_signal\n",
    "\n",
    "ny, nx = xgrid.shape\n",
    "nt = 4\n",
    "\n",
    "mapT = np.empty((ny,nx,nt))\n",
    "errT = np.empty((ny,nx,nt))\n",
    "\n",
    "for mm in range(0,4):\n",
    "    time1 = mdates.date2num(np.datetime64('2003-0' + str(mm+1) + '-15'))\n",
    "    time2 = time1 + epoch1 - epoch0\n",
    "    # call obj_map_2D\n",
    "    mapdata, err_rms = obj_map_2D(vart,var,smax,Ls,tmax,Lt,data,xgrid,ygrid,time2);\n",
    "    mapT[:,:,mm] = mapdata;\n",
    "    errT[:,:,mm] = err_rms;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19bfe4-8135-4d2e-b1d3-bc6e933ff917",
   "metadata": {},
   "source": [
    "## Fig 4. Plot the mapped data\n",
    "\n",
    "For one of the depth surfaces, plot the mapped data and error estimate.  Basic code is below.  Update for a better projection (e.g. Mercator).\n",
    "\n",
    "Be sure to annotate your figure with the choice of $L_s$, $L_t$ and the depth/pressure surface that you are mapping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3077036-9c04-4124-90d2-13eb6a232705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example of the mapped data.\n",
    "\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].pcolormesh(xgrid,ygrid,mapT[:,:,0])\n",
    "ax[0].set_xlabel('Longitude [$^\\circ$E]')\n",
    "ax[0].set_ylabel('Latitude [$^\\circ$N]')\n",
    "ax[0].set_title('Ls = ' + str(Ls) + 'km, Lt = ' + str(Lt) + 'd')\n",
    "ax[1].pcolormesh(xgrid,ygrid,errT[:,:,0])\n",
    "ax[1].set_xlabel('Longitude [$^\\circ$E]')\n",
    "ax[1].set_ylabel('Latitude [$^\\circ$N]')\n",
    "\n",
    "\n",
    "# Add the original Argo data over\n",
    "\n",
    "# Fix the colorbar.  Try to load something from https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b6cd9-0176-4125-be92-edf0e79acc68",
   "metadata": {},
   "source": [
    "## Fig 5. Add back in your mean and latitudinal dependence + plot\n",
    "\n",
    "Note that for the mapped data, you need to do this using your `xgrid` or `ygrid` and with the polynomial fit you identified above.\n",
    "\n",
    "Plot the mapped data and save as Fig 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414d719-5004-468d-8a77-191b2fdd67f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
